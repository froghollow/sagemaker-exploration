{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45cd728c",
   "metadata": {},
   "source": [
    "# PySpark & PyDeequ Shakeout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9de58e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: pydeequ in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (1.0.1)\n",
      "Requirement already satisfied: pandas>=0.23.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from pydeequ) (1.3.4)\n",
      "Requirement already satisfied: numpy>=1.14.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from pydeequ) (1.20.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from pandas>=0.23.0->pydeequ) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from pandas>=0.23.0->pydeequ) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas>=0.23.0->pydeequ) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip install pydeequ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb7d7230",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Please set env variable SPARK_VERSION\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/ec2-user/.ivy2/cache\n",
      "The jars for the packages stored in: /home/ec2-user/.ivy2/jars\n",
      "com.amazon.deequ#deequ added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-72f00b16-ce0a-40d6-86ca-2f00b504d91d;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.amazon.deequ#deequ;1.2.2-spark-3.0 in local-m2-cache\n",
      "\tfound org.scalanlp#breeze_2.12;0.13.2 in local-m2-cache\n",
      "\tfound org.scalanlp#breeze-macros_2.12;0.13.2 in local-m2-cache\n",
      "\tfound org.scala-lang#scala-reflect;2.12.1 in local-m2-cache\n",
      "\tfound com.github.fommil.netlib#core;1.1.2 in spark-list\n",
      "\tfound net.sf.opencsv#opencsv;2.3 in central\n",
      "\tfound com.github.rwl#jtransforms;2.4.0 in central\n",
      "\tfound junit#junit;4.8.2 in central\n",
      "\tfound org.apache.commons#commons-math3;3.2 in central\n",
      "\tfound org.spire-math#spire_2.12;0.13.0 in central\n",
      "\tfound org.spire-math#spire-macros_2.12;0.13.0 in central\n",
      "\tfound org.typelevel#machinist_2.12;0.6.1 in central\n",
      "\tfound com.chuusai#shapeless_2.12;2.3.2 in central\n",
      "\tfound org.typelevel#macro-compat_2.12;1.1.1 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.5 in central\n",
      ":: resolution report :: resolve 788ms :: artifacts dl 33ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazon.deequ#deequ;1.2.2-spark-3.0 from local-m2-cache in [default]\n",
      "\tcom.chuusai#shapeless_2.12;2.3.2 from central in [default]\n",
      "\tcom.github.fommil.netlib#core;1.1.2 from spark-list in [default]\n",
      "\tcom.github.rwl#jtransforms;2.4.0 from central in [default]\n",
      "\tjunit#junit;4.8.2 from central in [default]\n",
      "\tnet.sf.opencsv#opencsv;2.3 from central in [default]\n",
      "\torg.apache.commons#commons-math3;3.2 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.1 from local-m2-cache in [default]\n",
      "\torg.scalanlp#breeze-macros_2.12;0.13.2 from local-m2-cache in [default]\n",
      "\torg.scalanlp#breeze_2.12;0.13.2 from local-m2-cache in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.5 from central in [default]\n",
      "\torg.spire-math#spire-macros_2.12;0.13.0 from central in [default]\n",
      "\torg.spire-math#spire_2.12;0.13.0 from central in [default]\n",
      "\torg.typelevel#machinist_2.12;0.6.1 from central in [default]\n",
      "\torg.typelevel#macro-compat_2.12;1.1.1 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.scala-lang#scala-reflect;2.12.0 by [org.scala-lang#scala-reflect;2.12.1] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   16  |   0   |   0   |   1   ||   15  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-72f00b16-ce0a-40d6-86ca-2f00b504d91d\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 15 already retrieved (0kB/19ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/23 19:44:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/23 19:44:06 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/11/23 19:44:06 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "22/11/23 19:44:06 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "22/11/23 19:44:06 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, Row, DataFrame\n",
    "\n",
    "import sagemaker_pyspark\n",
    "import pydeequ\n",
    "\n",
    "classpath = \":\".join(sagemaker_pyspark.classpath_jars())\n",
    "\n",
    "spark = (SparkSession\n",
    "    .builder\n",
    "    .config(\"spark.driver.extraClassPath\", classpath)\n",
    "    .config(\"spark.jars.packages\", pydeequ.deequ_maven_coord)\n",
    "    .config(\"spark.jars.excludes\", pydeequ.f2j_maven_coord)\n",
    "    .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92056fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/JupyterSystemEnv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "openjdk version \"11.0.1\" 2018-10-16 LTS\n",
      "OpenJDK Runtime Environment Zulu11.2+3 (build 11.0.1+13-LTS)\n",
      "OpenJDK 64-Bit Server VM Zulu11.2+3 (build 11.0.1+13-LTS, mixed mode)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "echo $JAVA_HOME\n",
    "$JAVA_HOME/bin/java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a38eee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Record Date: timestamp (nullable = true)\n",
      " |-- Creditor Agency Name: string (nullable = true)\n",
      " |-- Agency ID: integer (nullable = true)\n",
      " |-- Agency Site Type Code: string (nullable = true)\n",
      " |-- Agency Site ID: string (nullable = true)\n",
      " |-- Agency Site Name: string (nullable = true)\n",
      " |-- Source Agency Alpha: string (nullable = true)\n",
      " |-- Source Agency Numeric: string (nullable = true)\n",
      " |-- Source Agency Site ID: string (nullable = true)\n",
      " |-- Agency Name: string (nullable = true)\n",
      " |-- Payment Agency ID: string (nullable = true)\n",
      " |-- Agency Type Code: string (nullable = true)\n",
      " |-- Payment Source Description: string (nullable = true)\n",
      " |-- Payment Source Code: string (nullable = true)\n",
      " |-- Payment Category Description: string (nullable = true)\n",
      " |-- Payment Type Description: string (nullable = true)\n",
      " |-- Payment Type Code: string (nullable = true)\n",
      " |-- Payment Type ID: integer (nullable = true)\n",
      " |-- Debt Type Code: string (nullable = true)\n",
      " |-- Debt Type Description: string (nullable = true)\n",
      " |-- Taxable Indicator: string (nullable = true)\n",
      " |-- Total Gross Amount: double (nullable = true)\n",
      " |-- Total Net Amount: double (nullable = true)\n",
      " |-- Total Gross Count: integer (nullable = true)\n",
      " |-- Total Net Count: integer (nullable = true)\n",
      " |-- Fiscal Year: integer (nullable = true)\n",
      " |-- Fiscal Quarter Number: integer (nullable = true)\n",
      " |-- Calendar Year: integer (nullable = true)\n",
      " |-- Calendar Quarter Number: integer (nullable = true)\n",
      " |-- Calendar Month Number: integer (nullable = true)\n",
      " |-- Calendar Day Number: integer (nullable = true)\n",
      "\n",
      "22/11/23 19:44:15 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "+-------------------+--------------------+---------+---------------------+--------------+--------------------+-------------------+---------------------+---------------------+--------------------+-----------------+----------------+--------------------------+-------------------+----------------------------+------------------------+-----------------+---------------+--------------+---------------------+-----------------+------------------+----------------+-----------------+---------------+-----------+---------------------+-------------+-----------------------+---------------------+-------------------+\n",
      "|        Record Date|Creditor Agency Name|Agency ID|Agency Site Type Code|Agency Site ID|    Agency Site Name|Source Agency Alpha|Source Agency Numeric|Source Agency Site ID|         Agency Name|Payment Agency ID|Agency Type Code|Payment Source Description|Payment Source Code|Payment Category Description|Payment Type Description|Payment Type Code|Payment Type ID|Debt Type Code|Debt Type Description|Taxable Indicator|Total Gross Amount|Total Net Amount|Total Gross Count|Total Net Count|Fiscal Year|Fiscal Quarter Number|Calendar Year|Calendar Quarter Number|Calendar Month Number|Calendar Day Number|\n",
      "+-------------------+--------------------+---------+---------------------+--------------+--------------------+-------------------+---------------------+---------------------+--------------------+-----------------+----------------+--------------------------+-------------------+----------------------------+------------------------+-----------------+---------------+--------------+---------------------+-----------------+------------------+----------------+-----------------+---------------+-----------+---------------------+-------------+-----------------------+---------------------+-------------------+\n",
      "|2022-10-01 00:00:00|Bureau of the Fis...|       83|              Federal|          33MK|33-MK-BUREAU OF F...|                 33|                   51|                   MK|Internal Revenue ...|              132|         Federal|                Tax Refund|                TAX|                         TDO|          Tax Refund EFT|               IE|             12|            FN|      FEDERAL NON-TAX|                N|             986.0|           986.0|                1|              1|       2023|                    1|         2022|                      4|                   10|                  1|\n",
      "|2022-10-01 00:00:00|Bureau of the Fis...|       83|              Federal|          33M7|33-M7-FISCAL SERV...|                 33|                   1C|                   M7|Department of Hom...|               21|           State|            Vendor payment|                VEN|                         TDO|              Vendor EFT|               VE|             13|            FN|      FEDERAL NON-TAX|                N|             289.6|           289.6|                5|              5|       2023|                    1|         2022|                      4|                   10|                  1|\n",
      "|2022-10-01 00:00:00|Bureau of the Fis...|       83|              Federal|          33M7|33-M7-FISCAL SERV...|                 33|                   18|                   M7|Office of Personn...|               17|         Federal|      Office of Personn...|                OPM|                         TDO|                 OPM EFT|               OE|              8|            FN|      FEDERAL NON-TAX|                N|          91000.38|        90170.87|             1351|           1340|       2023|                    1|         2022|                      4|                   10|                  1|\n",
      "|2022-10-01 00:00:00|Bureau of the Fis...|       83|              Federal|          33M7|33-M7-FISCAL SERV...|                 33|                   18|                   M7|Office of Personn...|               17|         Federal|      Office of Personn...|                OPM|                         TDO|               OPM Check|               OC|              8|            FN|      FEDERAL NON-TAX|                N|             992.8|          882.83|               13|             11|       2023|                    1|         2022|                      4|                   10|                  1|\n",
      "|2022-10-01 00:00:00|Bureau of the Fis...|       83|              Federal|          33M7|33-M7-FISCAL SERV...|                 33|                   03|                   M7|Dept. of Health &...|                3|         Federal|            Vendor payment|                VEN|                         TDO|              Vendor EFT|               VE|             13|            FN|      FEDERAL NON-TAX|                N|           1352.94|         1352.94|               19|             19|       2023|                    1|         2022|                      4|                   10|                  1|\n",
      "+-------------------+--------------------+---------+---------------------+--------------+--------------------+-------------------+---------------------+---------------------+--------------------+-----------------+----------------+--------------------------+-------------------+----------------------------+------------------------+-----------------+---------------+--------------+---------------------+-----------------+------------------+----------------+-----------------+---------------+-----------+---------------------+-------------+-----------------------+---------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File output: data/TOP_FedClct_20221001_20221001.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read a CSV file from local instance directory into a Spark dataframe\n",
    "file_input = 'data/TOP_FedClct_20221001_20221001.csv'\n",
    "\n",
    "df = spark.read \\\n",
    "    .format(\"com.databricks.spark.csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"delimiter\", \",\") \\\n",
    "    .load(file_input) \n",
    "\n",
    "df.printSchema()\n",
    "df.show(5)\n",
    "\n",
    "# Write a copy of the Spark dataframe to a Parquet output file\n",
    "file_output = file_input.replace('.csv', '.parquet')\n",
    "df.write.mode('overwrite').parquet(file_output)\n",
    "print(f'File output: {file_output}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c50b92e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Record Date: timestamp (nullable = true)\n",
      " |-- Creditor Agency Name: string (nullable = true)\n",
      " |-- Agency ID: integer (nullable = true)\n",
      " |-- Agency Site Type Code: string (nullable = true)\n",
      " |-- Agency Site ID: string (nullable = true)\n",
      " |-- Agency Site Name: string (nullable = true)\n",
      " |-- Source Agency Alpha: string (nullable = true)\n",
      " |-- Source Agency Numeric: string (nullable = true)\n",
      " |-- Source Agency Site ID: string (nullable = true)\n",
      " |-- Agency Name: string (nullable = true)\n",
      " |-- Payment Agency ID: string (nullable = true)\n",
      " |-- Agency Type Code: string (nullable = true)\n",
      " |-- Payment Source Description: string (nullable = true)\n",
      " |-- Payment Source Code: string (nullable = true)\n",
      " |-- Payment Category Description: string (nullable = true)\n",
      " |-- Payment Type Description: string (nullable = true)\n",
      " |-- Payment Type Code: string (nullable = true)\n",
      " |-- Payment Type ID: integer (nullable = true)\n",
      " |-- Debt Type Code: string (nullable = true)\n",
      " |-- Debt Type Description: string (nullable = true)\n",
      " |-- Taxable Indicator: string (nullable = true)\n",
      " |-- Total Gross Amount: double (nullable = true)\n",
      " |-- Total Net Amount: double (nullable = true)\n",
      " |-- Total Gross Count: integer (nullable = true)\n",
      " |-- Total Net Count: integer (nullable = true)\n",
      " |-- Fiscal Year: integer (nullable = true)\n",
      " |-- Fiscal Quarter Number: integer (nullable = true)\n",
      " |-- Calendar Year: integer (nullable = true)\n",
      " |-- Calendar Quarter Number: integer (nullable = true)\n",
      " |-- Calendar Month Number: integer (nullable = true)\n",
      " |-- Calendar Day Number: integer (nullable = true)\n",
      "\n",
      "+-------------------+--------------------+---------+---------------------+--------------+--------------------+-------------------+---------------------+---------------------+--------------------+-----------------+----------------+--------------------------+-------------------+----------------------------+------------------------+-----------------+---------------+--------------+---------------------+-----------------+------------------+----------------+-----------------+---------------+-----------+---------------------+-------------+-----------------------+---------------------+-------------------+\n",
      "|        Record Date|Creditor Agency Name|Agency ID|Agency Site Type Code|Agency Site ID|    Agency Site Name|Source Agency Alpha|Source Agency Numeric|Source Agency Site ID|         Agency Name|Payment Agency ID|Agency Type Code|Payment Source Description|Payment Source Code|Payment Category Description|Payment Type Description|Payment Type Code|Payment Type ID|Debt Type Code|Debt Type Description|Taxable Indicator|Total Gross Amount|Total Net Amount|Total Gross Count|Total Net Count|Fiscal Year|Fiscal Quarter Number|Calendar Year|Calendar Quarter Number|Calendar Month Number|Calendar Day Number|\n",
      "+-------------------+--------------------+---------+---------------------+--------------+--------------------+-------------------+---------------------+---------------------+--------------------+-----------------+----------------+--------------------------+-------------------+----------------------------+------------------------+-----------------+---------------+--------------+---------------------+-----------------+------------------+----------------+-----------------+---------------+-----------+---------------------+-------------+-----------------------+---------------------+-------------------+\n",
      "|2022-10-01 00:00:00|Bureau of the Fis...|       83|              Federal|          33MK|33-MK-BUREAU OF F...|                 33|                   51|                   MK|Internal Revenue ...|              132|         Federal|                Tax Refund|                TAX|                         TDO|          Tax Refund EFT|               IE|             12|            FN|      FEDERAL NON-TAX|                N|             986.0|           986.0|                1|              1|       2023|                    1|         2022|                      4|                   10|                  1|\n",
      "|2022-10-01 00:00:00|Bureau of the Fis...|       83|              Federal|          33M7|33-M7-FISCAL SERV...|                 33|                   1C|                   M7|Department of Hom...|               21|           State|            Vendor payment|                VEN|                         TDO|              Vendor EFT|               VE|             13|            FN|      FEDERAL NON-TAX|                N|             289.6|           289.6|                5|              5|       2023|                    1|         2022|                      4|                   10|                  1|\n",
      "|2022-10-01 00:00:00|Bureau of the Fis...|       83|              Federal|          33M7|33-M7-FISCAL SERV...|                 33|                   18|                   M7|Office of Personn...|               17|         Federal|      Office of Personn...|                OPM|                         TDO|                 OPM EFT|               OE|              8|            FN|      FEDERAL NON-TAX|                N|          91000.38|        90170.87|             1351|           1340|       2023|                    1|         2022|                      4|                   10|                  1|\n",
      "|2022-10-01 00:00:00|Bureau of the Fis...|       83|              Federal|          33M7|33-M7-FISCAL SERV...|                 33|                   18|                   M7|Office of Personn...|               17|         Federal|      Office of Personn...|                OPM|                         TDO|               OPM Check|               OC|              8|            FN|      FEDERAL NON-TAX|                N|             992.8|          882.83|               13|             11|       2023|                    1|         2022|                      4|                   10|                  1|\n",
      "|2022-10-01 00:00:00|Bureau of the Fis...|       83|              Federal|          33M7|33-M7-FISCAL SERV...|                 33|                   03|                   M7|Dept. of Health &...|                3|         Federal|            Vendor payment|                VEN|                         TDO|              Vendor EFT|               VE|             13|            FN|      FEDERAL NON-TAX|                N|           1352.94|         1352.94|               19|             19|       2023|                    1|         2022|                      4|                   10|                  1|\n",
      "+-------------------+--------------------+---------+---------------------+--------------+--------------------+-------------------+---------------------+---------------------+--------------------+-----------------+----------------+--------------------------+-------------------+----------------------------+------------------------+-----------------+---------------+--------------+---------------------+-----------------+------------------+----------------+-----------------+---------------+-----------+---------------------+-------------+-----------------------+---------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read a Parquet file from local instance directory\n",
    "file_input = 'data/TOP_FedClct_20221001_20221001.parquet'\n",
    "\n",
    "df = spark.read.parquet(file_input) \n",
    "df.printSchema()\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94f832e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: data/TOP_FedClct_20221001_20221001.csv to s3://wc2h-dtl-dev-datawork/rmyers07/data/TOP_FedClct_20221001_20221001.csv\n",
      "2022-11-23 19:44:22     920049 TOP_FedClct_20221001_20221001.csv\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# copy local CSV file to S3 using AWS CLI\n",
    "aws s3 cp data/TOP_FedClct_20221001_20221001.csv s3://wc2h-dtl-dev-datawork/rmyers07/data/TOP_FedClct_20221001_20221001.csv\n",
    "    \n",
    "aws s3 ls s3://wc2h-dtl-dev-datawork/rmyers07/data/TOP_FedClct_20221001_20221001.csv    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8b4aeb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record Date,Creditor Agency Name,Agency ID,Agency Site Type Code,Agency Site ID,Agency Site Name,Source Agency Alpha,Source Agency Numeric,Source Agency Site ID,Agency Name,Payment Agency ID,Agency Type Code,Payment Source Description,Payment Source Code,Payment Category Description,Payment Type Description,Payment Type Code,Payment Type ID,Debt Type Code,Debt Type Description,Taxable Indicator,Total Gross Amount,Total Net Amount,Total Gross Count,Total Net Count,Fiscal Year,Fiscal Quarter Number,Calendar Year,Calendar Quarter Number,Calendar Month Number,Calendar Day Number\n",
      "2022-10-01,Bureau of the Fiscal Service,83,Federal,33MK,33-MK-BUREAU OF FISCAL SERVICE,33,51,MK,Internal Revenue Service,132,Federal,Tax Refund,TAX,TDO,Tax Refund EFT,IE,12,FN,FEDERAL NON-TAX,N,986.00,986.00,1,1,2023,1,2022,4,10,01\n",
      "2022-10-01,Bureau of the Fiscal Service,83,Federal,33M7,33-M7-FISCAL SERVICE - DMSC,33,1C,M7,Department of Homeland Security,21,State,Vendor payment,VEN,TDO,Vendor EFT,VE,13,FN,FEDERAL NON-TAX,N,289.60,289.60,5,5,2023,1,2022,4,10,01\n",
      "2022-10-01,Bureau of the Fiscal Service,83,Federal,33M7,33-M7-FISCAL SERVICE - DMSC,33,18,M7,Office of Personnel Management,17,Federal,Office of Personnel Management,OPM,TDO,OPM EFT,OE,8,FN,FEDERAL NON-TAX,N,91000.38,90170.87,1351,1340,2023,1,2022,4,10,01\n",
      "2022-10-01,Bureau of the Fiscal Service,83,Federal,33M7,33-M7-FISCAL SERVICE - DMSC,33,18,M7,Office of Personnel Management,17,Federal,Office of Personnel Management,OPM,TDO,OPM Check,OC,8,FN,FEDERAL NON-TAX,N,992.80,882.83,13,11,2023,1,2022,4,10,01\n",
      "2022-10-01,Bureau of the Fiscal Service,83,Federal,33M7,33-M7-FISCAL SERVICE - DMSC,33,03,M7,Dept. of Health & Human Services,3,Federal,Vendor payment,VEN,TDO,Vendor EFT,VE,13,FN,FEDERAL NON-TAX,N,1352.94,1352.94,19,19,2023,1,2022,4,10,01\n",
      "2022-10-01,Bureau of the Fiscal Service,83,Federal,33M2,33-M2-DMSOC - EAST,33,27,M2,Social Security Administration,51,Federal,Social Security Administration,SSA,TDO,SSA EFT,SE,11,FN,FEDERAL NON-TAX,N,197.42,197.42,2,2,2023,1,2022,4,10,01\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file from S3 using Python AWS SDK\n",
    "import boto3\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "s3_bucket = 'wc2h-dtl-dev-datawork'\n",
    "s3_key = 'rmyers07/data/TOP_FedClct_20221001_20221001.csv'\n",
    "\n",
    "s3_obj = s3.get_object(Bucket=s3_bucket, Key=s3_key )\n",
    "data = s3_obj['Body'].read().decode('utf-8')\n",
    "\n",
    "for n,line in enumerate(data.split('\\n')):\n",
    "    print(line)\n",
    "    if n > 5:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a721fa87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/23 19:44:22 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "22/11/23 19:44:24 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: s3a://wc2h-dtl-dev-datawork/rmyers07/data/TOP_FedClct_20221001_20221001.csv.\n",
      "org.apache.hadoop.fs.s3a.AWSBadRequestException: getFileStatus on s3a://wc2h-dtl-dev-datawork/rmyers07/data/TOP_FedClct_20221001_20221001.csv: com.amazonaws.services.s3.model.AmazonS3Exception: The provided token is malformed or otherwise invalid. (Service: Amazon S3; Status Code: 400; Error Code: InvalidToken; Request ID: AAJW6XTJ98PRMM1F; S3 Extended Request ID: CnTZRfDUIxCOv7EVP3/PdxqTA0Bu6yMu3Y+1DltUYRhySDTc+pwfvGS7PLOFcxplXzjtBydZB+E=; Proxy: null), S3 Extended Request ID: CnTZRfDUIxCOv7EVP3/PdxqTA0Bu6yMu3Y+1DltUYRhySDTc+pwfvGS7PLOFcxplXzjtBydZB+E=:InvalidToken: The provided token is malformed or otherwise invalid. (Service: Amazon S3; Status Code: 400; Error Code: InvalidToken; Request ID: AAJW6XTJ98PRMM1F; S3 Extended Request ID: CnTZRfDUIxCOv7EVP3/PdxqTA0Bu6yMu3Y+1DltUYRhySDTc+pwfvGS7PLOFcxplXzjtBydZB+E=; Proxy: null)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:243)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:170)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3348)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3185)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.isDirectory(S3AFileSystem.java:4277)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:54)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:370)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)\n",
      "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "Caused by: com.amazonaws.services.s3.model.AmazonS3Exception: The provided token is malformed or otherwise invalid. (Service: Amazon S3; Status Code: 400; Error Code: InvalidToken; Request ID: AAJW6XTJ98PRMM1F; S3 Extended Request ID: CnTZRfDUIxCOv7EVP3/PdxqTA0Bu6yMu3Y+1DltUYRhySDTc+pwfvGS7PLOFcxplXzjtBydZB+E=; Proxy: null), S3 Extended Request ID: CnTZRfDUIxCOv7EVP3/PdxqTA0Bu6yMu3Y+1DltUYRhySDTc+pwfvGS7PLOFcxplXzjtBydZB+E=\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1828)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1412)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1374)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1145)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:802)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:770)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:744)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:704)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:686)\n",
      "\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:550)\n",
      "\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:530)\n",
      "\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5227)\n",
      "\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5173)\n",
      "\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5167)\n",
      "\tat com.amazonaws.services.s3.AmazonS3Client.listObjectsV2(AmazonS3Client.java:963)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$listObjects$7(S3AFileSystem.java:2116)\n",
      "\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:499)\n",
      "\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:412)\n",
      "\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:375)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.listObjects(S3AFileSystem.java:2107)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3322)\n",
      "\t... 21 more\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o51.load.\n: org.apache.hadoop.fs.s3a.AWSBadRequestException: getFileStatus on s3a://wc2h-dtl-dev-datawork/rmyers07/data/TOP_FedClct_20221001_20221001.csv: com.amazonaws.services.s3.model.AmazonS3Exception: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: FD3S30JKVRYGYVEY; S3 Extended Request ID: adfdllci1MHP4I80x4FTJT4C2GrxkrMZbKeGRURZk/XahCsJWF/yiZNJ5B34gUjqUOQR7NLbzzU=; Proxy: null), S3 Extended Request ID: adfdllci1MHP4I80x4FTJT4C2GrxkrMZbKeGRURZk/XahCsJWF/yiZNJ5B34gUjqUOQR7NLbzzU=:400 Bad Request: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: FD3S30JKVRYGYVEY; S3 Extended Request ID: adfdllci1MHP4I80x4FTJT4C2GrxkrMZbKeGRURZk/XahCsJWF/yiZNJ5B34gUjqUOQR7NLbzzU=; Proxy: null)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:243)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:170)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3286)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3185)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:3053)\n\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1760)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.exists(S3AFileSystem.java:4263)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:784)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:782)\n\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:372)\n\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n\tat scala.util.Success.map(Try.scala:213)\n\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1426)\n\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)\n\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)\n\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)\n\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)\n\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:177)\nCaused by: com.amazonaws.services.s3.model.AmazonS3Exception: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: FD3S30JKVRYGYVEY; S3 Extended Request ID: adfdllci1MHP4I80x4FTJT4C2GrxkrMZbKeGRURZk/XahCsJWF/yiZNJ5B34gUjqUOQR7NLbzzU=; Proxy: null), S3 Extended Request ID: adfdllci1MHP4I80x4FTJT4C2GrxkrMZbKeGRURZk/XahCsJWF/yiZNJ5B34gUjqUOQR7NLbzzU=\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1828)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1412)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1374)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1145)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:802)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:770)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:744)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:704)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:686)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:550)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:530)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5227)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5173)\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1360)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$6(S3AFileSystem.java:2066)\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:412)\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:375)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2056)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2032)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3273)\n\t... 20 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11398/506088905.py\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0ms3_url_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m's3a://wc2h-dtl-dev-datawork/rmyers07/data/TOP_FedClct_20221001_20221001.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"com.databricks.spark.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.8/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.8/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o51.load.\n: org.apache.hadoop.fs.s3a.AWSBadRequestException: getFileStatus on s3a://wc2h-dtl-dev-datawork/rmyers07/data/TOP_FedClct_20221001_20221001.csv: com.amazonaws.services.s3.model.AmazonS3Exception: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: FD3S30JKVRYGYVEY; S3 Extended Request ID: adfdllci1MHP4I80x4FTJT4C2GrxkrMZbKeGRURZk/XahCsJWF/yiZNJ5B34gUjqUOQR7NLbzzU=; Proxy: null), S3 Extended Request ID: adfdllci1MHP4I80x4FTJT4C2GrxkrMZbKeGRURZk/XahCsJWF/yiZNJ5B34gUjqUOQR7NLbzzU=:400 Bad Request: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: FD3S30JKVRYGYVEY; S3 Extended Request ID: adfdllci1MHP4I80x4FTJT4C2GrxkrMZbKeGRURZk/XahCsJWF/yiZNJ5B34gUjqUOQR7NLbzzU=; Proxy: null)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:243)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:170)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3286)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3185)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:3053)\n\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1760)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.exists(S3AFileSystem.java:4263)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:784)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:782)\n\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:372)\n\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n\tat scala.util.Success.map(Try.scala:213)\n\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1426)\n\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)\n\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)\n\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)\n\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)\n\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:177)\nCaused by: com.amazonaws.services.s3.model.AmazonS3Exception: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: FD3S30JKVRYGYVEY; S3 Extended Request ID: adfdllci1MHP4I80x4FTJT4C2GrxkrMZbKeGRURZk/XahCsJWF/yiZNJ5B34gUjqUOQR7NLbzzU=; Proxy: null), S3 Extended Request ID: adfdllci1MHP4I80x4FTJT4C2GrxkrMZbKeGRURZk/XahCsJWF/yiZNJ5B34gUjqUOQR7NLbzzU=\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1828)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1412)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1374)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1145)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:802)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:770)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:744)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:704)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:686)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:550)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:530)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5227)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5173)\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1360)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$6(S3AFileSystem.java:2066)\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:412)\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:375)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2056)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2032)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3273)\n\t... 20 more\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file from S3 into a Spark data frame\n",
    "s3_url_input = 's3a://wc2h-dtl-dev-datawork/rmyers07/data/TOP_FedClct_20221001_20221001.csv'\n",
    "\n",
    "df = spark.read \\\n",
    "    .format(\"com.databricks.spark.csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"delimiter\", \",\") \\\n",
    "    .load(s3_url_input) \n",
    "\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41ae2b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'# Use AWS CLI to list objects in publicly-accessible S3 parquet data set (AWS US East Region)\\naws s3 ls s3://amazon-reviews-pds/parquet/product_category=Electronics/\\n\\n# WC2H fails , DAAB LAB works \\n'' returned non-zero exit status 255.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11398/511600093.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bash'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'# Use AWS CLI to list objects in publicly-accessible S3 parquet data set (AWS US East Region)\\naws s3 ls s3://amazon-reviews-pds/parquet/product_category=Electronics/\\n\\n# WC2H fails , DAAB LAB works \\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2460\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2461\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2462\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2463\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.8/site-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mnamed_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshebang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# write a basic docstring:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.8/site-packages/decorator.py\u001b[0m in \u001b[0;36mfun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkwsyntax\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcaller\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextras\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m     \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.8/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.8/site-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_error\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_close\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'# Use AWS CLI to list objects in publicly-accessible S3 parquet data set (AWS US East Region)\\naws s3 ls s3://amazon-reviews-pds/parquet/product_category=Electronics/\\n\\n# WC2H fails , DAAB LAB works \\n'' returned non-zero exit status 255."
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Use AWS CLI to list objects in publicly-accessible S3 parquet data set (AWS US East Region)\n",
    "aws s3 ls s3://amazon-reviews-pds/parquet/product_category=Electronics/\n",
    "\n",
    "# WC2H fails , DAAB LAB works "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2926df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/23 19:45:41 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: s3a://amazon-reviews-pds/parquet/product_category=Electronics/.\n",
      "org.apache.hadoop.fs.s3a.AWSBadRequestException: getFileStatus on s3a://amazon-reviews-pds/parquet/product_category=Electronics: com.amazonaws.services.s3.model.AmazonS3Exception: The provided token is malformed or otherwise invalid. (Service: Amazon S3; Status Code: 400; Error Code: InvalidToken; Request ID: XCK6EDDR4PK13KMT; S3 Extended Request ID: IAoh8StamstsSJFIXMpuT+fkAW2/4fkyfl1fnHSUCDw2fOXEegRZCbbT2Xez/hJfToYqTlYR3cClg9fajHY+CQ==; Proxy: null), S3 Extended Request ID: IAoh8StamstsSJFIXMpuT+fkAW2/4fkyfl1fnHSUCDw2fOXEegRZCbbT2Xez/hJfToYqTlYR3cClg9fajHY+CQ==:InvalidToken: The provided token is malformed or otherwise invalid. (Service: Amazon S3; Status Code: 400; Error Code: InvalidToken; Request ID: XCK6EDDR4PK13KMT; S3 Extended Request ID: IAoh8StamstsSJFIXMpuT+fkAW2/4fkyfl1fnHSUCDw2fOXEegRZCbbT2Xez/hJfToYqTlYR3cClg9fajHY+CQ==; Proxy: null)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:243)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:170)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3348)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3185)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.isDirectory(S3AFileSystem.java:4277)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:54)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:370)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)\n",
      "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\n",
      "\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:562)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "Caused by: com.amazonaws.services.s3.model.AmazonS3Exception: The provided token is malformed or otherwise invalid. (Service: Amazon S3; Status Code: 400; Error Code: InvalidToken; Request ID: XCK6EDDR4PK13KMT; S3 Extended Request ID: IAoh8StamstsSJFIXMpuT+fkAW2/4fkyfl1fnHSUCDw2fOXEegRZCbbT2Xez/hJfToYqTlYR3cClg9fajHY+CQ==; Proxy: null), S3 Extended Request ID: IAoh8StamstsSJFIXMpuT+fkAW2/4fkyfl1fnHSUCDw2fOXEegRZCbbT2Xez/hJfToYqTlYR3cClg9fajHY+CQ==\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1828)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1412)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1374)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1145)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:802)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:770)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:744)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:704)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:686)\n",
      "\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:550)\n",
      "\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:530)\n",
      "\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5227)\n",
      "\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5173)\n",
      "\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5167)\n",
      "\tat com.amazonaws.services.s3.AmazonS3Client.listObjectsV2(AmazonS3Client.java:963)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$listObjects$7(S3AFileSystem.java:2116)\n",
      "\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:499)\n",
      "\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:412)\n",
      "\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:375)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.listObjects(S3AFileSystem.java:2107)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3322)\n",
      "\t... 21 more\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o54.parquet.\n: org.apache.hadoop.fs.s3a.AWSBadRequestException: getFileStatus on s3a://amazon-reviews-pds/parquet/product_category=Electronics: com.amazonaws.services.s3.model.AmazonS3Exception: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: 0PZZKCKGXGBFWJ40; S3 Extended Request ID: 4zZBiQ5h8tYDEVBgfAk/8WiJqbh6ZT/lrgAFd1jAJGlnhrztRgOLh4N3lb1i0wp/x49zxxFpQEctikVMAg8VAw==; Proxy: null), S3 Extended Request ID: 4zZBiQ5h8tYDEVBgfAk/8WiJqbh6ZT/lrgAFd1jAJGlnhrztRgOLh4N3lb1i0wp/x49zxxFpQEctikVMAg8VAw==:400 Bad Request: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: 0PZZKCKGXGBFWJ40; S3 Extended Request ID: 4zZBiQ5h8tYDEVBgfAk/8WiJqbh6ZT/lrgAFd1jAJGlnhrztRgOLh4N3lb1i0wp/x49zxxFpQEctikVMAg8VAw==; Proxy: null)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:243)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:170)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3286)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3185)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:3053)\n\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1760)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.exists(S3AFileSystem.java:4263)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:784)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:782)\n\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:372)\n\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n\tat scala.util.Success.map(Try.scala:213)\n\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1426)\n\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)\n\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)\n\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)\n\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)\n\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:177)\nCaused by: com.amazonaws.services.s3.model.AmazonS3Exception: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: 0PZZKCKGXGBFWJ40; S3 Extended Request ID: 4zZBiQ5h8tYDEVBgfAk/8WiJqbh6ZT/lrgAFd1jAJGlnhrztRgOLh4N3lb1i0wp/x49zxxFpQEctikVMAg8VAw==; Proxy: null), S3 Extended Request ID: 4zZBiQ5h8tYDEVBgfAk/8WiJqbh6ZT/lrgAFd1jAJGlnhrztRgOLh4N3lb1i0wp/x49zxxFpQEctikVMAg8VAw==\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1828)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1412)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1374)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1145)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:802)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:770)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:744)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:704)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:686)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:550)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:530)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5227)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5173)\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1360)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$6(S3AFileSystem.java:2066)\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:412)\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:375)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2056)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2032)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3273)\n\t... 20 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11398/692615999.py\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Read a publicly-accessible S3 parquet data set into a Spark dataframe (AWS US East Region)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"s3a://amazon-reviews-pds/parquet/product_category=Electronics/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.8/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, *paths, **options)\u001b[0m\n\u001b[1;32m    362\u001b[0m         )\n\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m     def text(\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.8/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o54.parquet.\n: org.apache.hadoop.fs.s3a.AWSBadRequestException: getFileStatus on s3a://amazon-reviews-pds/parquet/product_category=Electronics: com.amazonaws.services.s3.model.AmazonS3Exception: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: 0PZZKCKGXGBFWJ40; S3 Extended Request ID: 4zZBiQ5h8tYDEVBgfAk/8WiJqbh6ZT/lrgAFd1jAJGlnhrztRgOLh4N3lb1i0wp/x49zxxFpQEctikVMAg8VAw==; Proxy: null), S3 Extended Request ID: 4zZBiQ5h8tYDEVBgfAk/8WiJqbh6ZT/lrgAFd1jAJGlnhrztRgOLh4N3lb1i0wp/x49zxxFpQEctikVMAg8VAw==:400 Bad Request: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: 0PZZKCKGXGBFWJ40; S3 Extended Request ID: 4zZBiQ5h8tYDEVBgfAk/8WiJqbh6ZT/lrgAFd1jAJGlnhrztRgOLh4N3lb1i0wp/x49zxxFpQEctikVMAg8VAw==; Proxy: null)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:243)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:170)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3286)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3185)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:3053)\n\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1760)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.exists(S3AFileSystem.java:4263)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:784)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:782)\n\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:372)\n\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n\tat scala.util.Success.map(Try.scala:213)\n\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1426)\n\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)\n\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)\n\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)\n\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)\n\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:177)\nCaused by: com.amazonaws.services.s3.model.AmazonS3Exception: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: 0PZZKCKGXGBFWJ40; S3 Extended Request ID: 4zZBiQ5h8tYDEVBgfAk/8WiJqbh6ZT/lrgAFd1jAJGlnhrztRgOLh4N3lb1i0wp/x49zxxFpQEctikVMAg8VAw==; Proxy: null), S3 Extended Request ID: 4zZBiQ5h8tYDEVBgfAk/8WiJqbh6ZT/lrgAFd1jAJGlnhrztRgOLh4N3lb1i0wp/x49zxxFpQEctikVMAg8VAw==\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1828)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1412)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1374)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1145)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:802)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:770)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:744)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:704)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:686)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:550)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:530)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5227)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5173)\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1360)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$6(S3AFileSystem.java:2066)\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:412)\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:375)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2056)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2032)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3273)\n\t... 20 more\n"
     ]
    }
   ],
   "source": [
    "# Read a publicly-accessible S3 parquet data set into a Spark dataframe (AWS US East Region)\n",
    "df = spark.read.parquet(\"s3a://amazon-reviews-pds/parquet/product_category=Electronics/\")\n",
    "df.printSchema()\n",
    "df.show(5)\n",
    "\n",
    "# WC2H fails , DAAB LAB works "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
